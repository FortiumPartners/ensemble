# Eval spec for /create-prd command quality assessment
# TRD Task: TRD-TEST-4B (Command Evals)
# AC Reference: Consolidated from TRD-TEST-056/057

name: create-prd
version: 2.0.0
description: |
  Evaluate quality of PRDs generated from PROJECT.md inputs.
  Compares quality between structured input (PROJECT.md) vs minimal description.

  Uses variant-based fixture structure for proper A/B testing:
  - with_project_md variant: Full framework with PROJECT.md input
  - minimal_input variant: Baseline with brief description only

# Test fixture configuration (per-variant fixture paths)
fixture:
  repo: ensemble-vnext-test-fixtures
  # Note: path is now per-variant, see variants[].fixture_path

# Test case definition
test_case:
  base_prompt: |
    Generate a Product Requirements Document (PRD) for a task management API.
    The PRD should be comprehensive and ready for TRD creation.

    Output the PRD to docs/PRD/.

# Variants to compare using variant-based fixture structure
variants:
  - id: with_project_md
    name: With PROJECT.md Input
    description: Generate PRD from structured PROJECT.md specification
    fixture_path: variants/full/taskflow-api
    suffix: |
      Run /create-prd using the PROJECT.md in this directory as input.

      The PROJECT.md contains:
      - User stories for task management
      - Tech stack specification
      - Deployment requirements
      - Success criteria

      Use the structured PROJECT.md to generate a complete PRD.
    skill_enabled: true
    agent_enabled: true
    timeout_seconds: 600

  - id: minimal_input
    name: Minimal Description Input (Baseline)
    description: Generate PRD from brief description only
    fixture_path: variants/baseline/taskflow-api
    is_baseline: true
    suffix: |
      Create a PRD for a task management API with:
      - User authentication (JWT)
      - Kanban boards with drag-and-drop
      - Team workspaces
      - Task assignment and prioritization

      Output the PRD to docs/PRD/taskflow.md

      Do NOT read PROJECT.md - use only the description above.
    skill_enabled: false
    agent_enabled: false
    timeout_seconds: 600

# Runs per variant for statistical significance
runs_per_variant: 5

# Binary checks (deterministic pass/fail)
binary_checks:
  - name: prd_created
    description: PRD file was created in docs/PRD/
    check: |
      test -f docs/PRD/*.md
    weight: 0.30

  - name: has_overview
    description: Contains Overview/Summary section
    check: |
      grep -qiE "^#+.*(overview|summary|introduction)" docs/PRD/*.md
    weight: 0.20

  - name: has_user_stories
    description: Contains User Stories/Features section
    check: |
      grep -qiE "^#+.*(user stor|features|requirements)" docs/PRD/*.md
    weight: 0.25

  - name: has_success_criteria
    description: Contains Success Criteria/Acceptance section
    check: |
      grep -qiE "^#+.*(success|acceptance|criteria|metrics)" docs/PRD/*.md
    weight: 0.25

# LLM-judged quality metrics
metrics:
  - name: prd_quality
    description: Overall PRD quality assessment
    rubric: prd-quality.md
    weight: 1.0
    files:
      - "docs/PRD/*.md"
    context_files:
      - "PROJECT.md"

# Judge configuration
judge:
  model: opus
  use_cot: true
  temperature: 0

# Execution configuration
execution:
  timeout: 600
  parallel: true
  cleanup: true

# Acceptance criteria
acceptance:
  minimum_mean_difference: 0.5
  significance_threshold: 0.05
  binary_pass_threshold: 0.9

# Metadata for reporting
metadata:
  category: command
  target_command: create-prd
  complexity: intermediate
  estimated_duration_minutes: 10
  tags:
    - command-eval
    - prd
    - document-generation
