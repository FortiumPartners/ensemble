# Eval spec for pytest skill A/B comparison
# TRD Task: TRD-TEST-079
# AC Reference: AC-EF2, AC-SK1, AC-SK2

name: pytest
version: 2.0.0
description: |
  Evaluate effectiveness of the pytest skill for test generation tasks.
  Compares quality outcomes between sessions with skill loaded vs without skill.
  Focus on test structure, coverage patterns, and pytest-specific best practices.

  Uses variant-based fixture structure for proper A/B testing:
  - with_skill variant: Full framework with pytest skill enabled
  - without_skill variant: Baseline without skills

# Test fixture configuration (per-variant fixture paths)
fixture:
  repo: ensemble-vnext-test-fixtures
  # Note: path is now per-variant, see variants[].fixture_path

# Test case definition
test_case:
  base_prompt: |
    Write comprehensive tests for a string utility module that includes functions
    for capitalizing words, reversing strings, and counting vowels.
    Achieve at least 90% coverage.

    Requirements:
    - Create the string utility module (string_utils.py) with functions:
      - capitalize_words(text: str) -> str: Capitalize first letter of each word
      - reverse_string(text: str) -> str: Reverse the input string
      - count_vowels(text: str) -> int: Count vowels (a, e, i, o, u) in text
    - Write comprehensive pytest tests (test_string_utils.py) that:
      - Test happy path for each function
      - Test edge cases (empty strings, special characters, numbers)
      - Test boundary values and unusual inputs
      - Use pytest fixtures for shared test data
      - Include parametrized tests where appropriate
    - Include pytest-cov configuration for coverage measurement

# Variants to compare using variant-based fixture structure
variants:
  - id: with_skill
    name: With Skill
    description: Use pytest skill for test implementation
    fixture_path: variants/full/pytest-tests
    suffix: |
      Use the Skill tool to invoke the pytest skill before starting implementation.
      After loading the skill, follow its patterns and best practices for test structure,
      fixtures, parametrization, and coverage configuration.
    skill_enabled: true
    agent_enabled: true
    timeout_seconds: 300
    expected_skills:
      - pytest
      - developing-with-python

  - id: without_skill
    name: Without Skill (Baseline)
    description: Implement without skill guidance
    fixture_path: variants/baseline/pytest-tests
    is_baseline: true
    suffix: ""
    skill_enabled: false
    agent_enabled: false
    timeout_seconds: 300
    expected_skills: []

# Execution configuration
runs_per_variant: 5

# Automated binary checks (shell-based, exit 0 on success)
binary_checks:
  - name: tests_executable
    description: Generated tests can be executed with pytest
    check: |
      cd {workspace} && python -m pytest --collect-only 2>&1 | grep -E "test session starts|collected"
    weight: 0.4

  - name: coverage_present
    description: Tests include coverage measurement setup
    check: |
      count=0
      # Check for pytest-cov usage in test files or config
      count=$((count + $(grep -l "pytest-cov\|--cov\|coverage" {workspace}/*.py {workspace}/pyproject.toml {workspace}/setup.cfg {workspace}/pytest.ini 2>/dev/null | wc -l)))
      # Also check for coverage imports or decorators
      count=$((count + $(grep -E "import coverage|@pytest.mark.cov|cov_report" {workspace}/*.py 2>/dev/null | wc -l)))
      [ "$count" -ge 1 ]
    weight: 0.3

  - name: fixtures_used
    description: Code uses pytest fixtures for test setup
    check: |
      count=$(grep -E '@pytest\.fixture|def fixture|yield|request\.param' {workspace}/test_*.py {workspace}/*_test.py 2>/dev/null | wc -l)
      [ "$count" -ge 1 ]
    weight: 0.15

  - name: parametrization_used
    description: Code uses pytest parametrization for multiple test cases
    check: |
      count=$(grep -E '@pytest\.mark\.parametrize|pytest\.param' {workspace}/test_*.py {workspace}/*_test.py 2>/dev/null | wc -l)
      [ "$count" -ge 1 ]
    weight: 0.15

# LLM-judged quality metrics
metrics:
  - name: test_quality
    description: Overall test quality assessment including coverage, structure, assertions, and best practices
    rubric: test-quality.md
    files:
      - "test_*.py"
      - "*_test.py"
      - "tests/**/*.py"
    context_files:
      - "*.py"
      - "!test_*.py"
      - "!*_test.py"
    weight: 1.0

# Judge configuration
judge:
  model: opus
  use_cot: true
  temperature: 0
  system_prompt: |
    You are evaluating pytest test code quality for a string utility module.
    Focus on pytest-specific patterns: fixtures, parametrization, markers, and idiomatic assertions.
    Score the test_quality metric on a 1-5 scale according to the provided rubric.
    Provide brief justification for your score.
    Output your response as JSON with structure: {"scores": {"test_quality": {"score": N, "justification": "...", "dimension_scores": {"coverage": N, "structure": N, "assertions": N, "best_practices": N}}}}

# Acceptance criteria for this eval
acceptance:
  # Minimum mean score difference between variants to be considered significant
  minimum_mean_difference: 0.5
  # Statistical significance threshold (p-value)
  significance_threshold: 0.05
  # Minimum passing rate for binary checks
  binary_pass_threshold: 0.8

# Metadata for reporting
metadata:
  category: skill
  target_skill: pytest
  language: python
  complexity: intermediate
  estimated_duration_minutes: 5
  tags:
    - testing
    - pytest
    - fixtures
    - parametrization
    - coverage
    - tdd
