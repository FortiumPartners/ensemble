# Eval spec for developing-with-python skill A/B comparison
# TRD Task: TRD-TEST-075
# AC Reference: AC-EF2, AC-SK1, AC-SK2

name: developing-with-python
version: 2.0.0
description: |
  Evaluate effectiveness of the developing-with-python skill for CLI calculator implementation.
  Compares quality outcomes between sessions with skill loaded vs without skill.

  Uses variant-based fixture structure for proper A/B testing:
  - with_skill variant: Full framework with skills enabled
  - without_skill variant: Baseline without skills

# Test fixture configuration (per-variant fixture paths)
fixture:
  repo: ensemble-vnext-test-fixtures
  # Note: path is now per-variant, see variants[].fixture_path

# Test case definition
test_case:
  base_prompt: |
    Build a CLI calculator that supports addition, subtraction, multiplication, and division.
    Include argument parsing and helpful error messages.

    Requirements:
    - Accept command line arguments: <number1> <operator> <number2>
    - Support operators: +, -, *, /
    - Handle division by zero gracefully
    - Provide clear error messages for invalid input
    - Include type hints throughout the code
    - Write comprehensive tests using pytest

# Variants to compare using variant-based fixture structure
variants:
  - id: with_skill
    name: With Skill
    description: Use developing-with-python skill for implementation
    fixture_path: variants/full/python-cli
    suffix: |
      Use the Skill tool to invoke the developing-with-python skill before starting implementation.
      After loading the skill, follow its patterns and best practices.
    skill_enabled: true
    agent_enabled: true
    timeout_seconds: 300
    expected_skills:
      - developing-with-python
      - pytest

  - id: without_skill
    name: Without Skill (Baseline)
    description: Implement without skill guidance
    fixture_path: variants/baseline/python-cli
    is_baseline: true
    suffix: ""
    skill_enabled: false
    agent_enabled: false
    timeout_seconds: 300
    expected_skills: []

# Execution configuration
runs_per_variant: 5

# Automated binary checks (shell-based, exit 0 on success)
binary_checks:
  - name: tests_pass
    description: Generated tests execute successfully with pytest
    check: |
      cd {workspace} && python -m pytest -v --tb=short 2>&1
    weight: 0.4

  - name: type_hints
    description: Code includes type hints on functions (minimum 3 occurrences)
    check: |
      count=$(grep -E 'def .+\(.+:.+\).*->|: (str|int|float|bool|list|dict|None)' {workspace}/*.py 2>/dev/null | wc -l)
      [ "$count" -ge 3 ]
    weight: 0.3

  - name: error_handling
    description: Code includes try/except blocks for error handling
    check: |
      count=$(grep -E 'try:|except .+:' {workspace}/*.py 2>/dev/null | wc -l)
      [ "$count" -ge 2 ]
    weight: 0.3

# LLM-judged quality metrics
metrics:
  - name: code_quality
    description: Overall code quality assessment
    rubric: code-quality.md
    files:
      - "*.py"
      - "!test_*.py"
      - "!*_test.py"
    weight: 0.35

  - name: test_quality
    description: Test coverage and structure assessment
    rubric: test-quality.md
    files:
      - "test_*.py"
      - "*_test.py"
    context_files:
      - "*.py"
      - "!test_*.py"
    weight: 0.35

  - name: error_handling_quality
    description: Error handling patterns and graceful degradation
    rubric: error-handling.md
    files:
      - "*.py"
      - "!test_*.py"
    weight: 0.30

# Judge configuration
judge:
  model: opus
  use_cot: true
  temperature: 0
  system_prompt: |
    You are evaluating Python code quality for a CLI calculator implementation.
    Score each metric on a 1-5 scale according to the provided rubric.
    Provide brief justification for each score.
    Output your response as JSON with structure: {"scores": {"metric_name": {"score": N, "justification": "..."}}}

# Acceptance criteria for this eval
acceptance:
  # Minimum mean score difference between variants to be considered significant
  minimum_mean_difference: 0.5
  # Statistical significance threshold (p-value)
  significance_threshold: 0.05
  # Minimum passing rate for binary checks
  binary_pass_threshold: 0.8

# Metadata for reporting
metadata:
  category: skill
  target_skill: developing-with-python
  language: python
  complexity: beginner
  estimated_duration_minutes: 5
  tags:
    - cli
    - calculator
    - pytest
    - type-hints
