# Eval spec for code-reviewer agent A/B comparison
# TRD Task: TRD-TEST-083
# AC Reference: AC-EF3, AC-A4, AC-A6

name: code-reviewer
version: 2.0.0
description: |
  Evaluate effectiveness of the code-reviewer agent for comprehensive code review tasks.
  Compares quality outcomes between sessions with @code-reviewer delegation vs direct review.

  The code-reviewer agent specializes in:
  - Security review (OWASP Top 10)
  - Code quality assessment
  - Best practices enforcement
  - Definition of Done verification
  - Constructive, actionable feedback

  Uses variant-based fixture structure for proper A/B testing:
  - with_agent variant: Full framework with code-reviewer agent delegation
  - without_agent variant: Baseline without agent delegation

# Test fixture configuration (per-variant fixture paths)
fixture:
  repo: ensemble-vnext-test-fixtures
  # Note: path is now per-variant, see variants[].fixture_path

# Test case definition
test_case:
  base_prompt: |
    Perform a comprehensive code review of the calculator implementation in this directory.

    Your review should cover:
    1. Security issues (if any) - check for injection vulnerabilities, input validation
    2. Code quality concerns - readability, maintainability, complexity
    3. Potential bugs or edge cases that could cause failures
    4. Best practices violations specific to Python
    5. Suggestions for improvement with specific recommendations

    For each issue found:
    - Reference the specific file and line number where applicable
    - Explain WHY it's an issue (not just WHAT)
    - Provide a concrete recommendation for fixing it

    At the end, provide an overall assessment:
    - Rate the code quality on a scale of 1-5
    - List the top 3 most important improvements
    - Indicate whether this code is APPROVED, APPROVED WITH RECOMMENDATIONS, or BLOCKED

# Variants to compare using variant-based fixture structure
variants:
  - id: with_agent
    name: With Agent
    description: Delegate code review to @code-reviewer agent
    fixture_path: variants/full/python-cli
    suffix: |
      Delegate this code review task to the @code-reviewer agent.
      The agent specializes in security review (OWASP Top 10), quality assessment,
      and providing structured, actionable feedback.

      Ensure the review includes:
      - Security Review section (PASS/FAIL/WARN for each category)
      - Code Quality section
      - Final verdict with clear rationale
    agent_enabled: true
    skill_enabled: true
    timeout_seconds: 300
    expected_agents:
      - code-reviewer

  - id: without_agent
    name: Without Agent (Baseline)
    description: Complete code review directly without agent delegation
    fixture_path: variants/baseline/python-cli
    is_baseline: true
    suffix: |
      Complete this code review task directly without delegating to any agent.
      Provide your analysis directly based on the code in this directory.
    agent_enabled: false
    skill_enabled: false
    timeout_seconds: 300
    expected_agents: []

# Execution configuration
runs_per_variant: 5

# Automated binary checks (shell-based, exit 0 on success)
binary_checks:
  - name: issues_identified
    description: Review identifies at least 3 specific issues with code references
    check: |
      # Count mentions of specific issues (line references, issue keywords)
      review_output=$(cat {workspace}/review_output.txt 2>/dev/null || echo "{last_response}")
      issue_count=$(echo "$review_output" | grep -iE '(line [0-9]+|:[0-9]+|issue|problem|bug|concern|vulnerability|improvement|recommendation)' | wc -l)
      [ "$issue_count" -ge 3 ]
    weight: 0.4

  - name: security_coverage
    description: Review addresses security considerations
    check: |
      review_output=$(cat {workspace}/review_output.txt 2>/dev/null || echo "{last_response}")
      # Check for security-related terms
      security_mentions=$(echo "$review_output" | grep -iE '(security|injection|validation|sanitiz|auth|OWASP|XSS|SQL|input.*(valid|check)|untrusted)' | wc -l)
      [ "$security_mentions" -ge 1 ]
    weight: 0.3

  - name: actionable_feedback
    description: Review provides specific actionable recommendations
    check: |
      review_output=$(cat {workspace}/review_output.txt 2>/dev/null || echo "{last_response}")
      # Check for recommendation patterns
      recommendation_count=$(echo "$review_output" | grep -iE '(should|recommend|suggest|consider|instead|better to|improve by|could be|would be better)' | wc -l)
      [ "$recommendation_count" -ge 3 ]
    weight: 0.3

# LLM-judged quality metrics
metrics:
  - name: review_quality
    description: Overall quality and thoroughness of the code review
    rubric: code-quality.md
    custom_prompt: |
      You are evaluating the quality of a CODE REVIEW, not the code itself.

      Assess the code review output on these dimensions:

      **Thoroughness (30%)**
      - Does the review cover security, quality, and best practices?
      - Are multiple aspects of the code examined?
      - Is the coverage comprehensive or superficial?

      **Specificity (25%)**
      - Are issues identified with specific file/line references?
      - Are problems clearly described with context?
      - Is it clear exactly what needs to be changed?

      **Actionability (25%)**
      - Are recommendations concrete and implementable?
      - Does the review explain HOW to fix issues, not just WHAT?
      - Could a developer act on this feedback immediately?

      **Professionalism (20%)**
      - Is feedback constructive rather than harsh?
      - Is the review well-organized and easy to follow?
      - Is the final verdict clear and justified?

      Score 1-5 based on these criteria:
      - 5: Exceptional review - thorough, specific, actionable, professional
      - 4: Good review - covers most areas well, minor gaps
      - 3: Adequate review - functional but missing depth or specificity
      - 2: Below average - superficial or missing key areas
      - 1: Poor review - vague, unhelpful, or unprofessional
    files:
      - "review_output.txt"
    use_last_response: true
    weight: 0.5

  - name: security_depth
    description: Depth and accuracy of security assessment
    rubric: code-quality.md
    custom_prompt: |
      Evaluate the SECURITY REVIEW component specifically.

      Consider:
      - Does the review identify relevant security concerns for the code type?
      - For a CLI calculator, relevant concerns include:
        - Input validation (handling malformed input)
        - Command injection (if user input is used in shell commands)
        - Error handling (information disclosure in errors)
        - Division by zero handling

      Score based on:
      - 5: Comprehensive security analysis with OWASP-aligned categories
      - 4: Good security coverage with specific findings
      - 3: Basic security mentions with some specific concerns
      - 2: Superficial security discussion, misses obvious issues
      - 1: No meaningful security analysis
    files:
      - "review_output.txt"
    use_last_response: true
    weight: 0.25

  - name: feedback_quality
    description: Quality and constructiveness of feedback provided
    rubric: code-quality.md
    custom_prompt: |
      Evaluate the quality of FEEDBACK and RECOMMENDATIONS in this code review.

      Strong feedback characteristics:
      - Explains the "why" behind recommendations
      - Provides specific code examples or fixes
      - Prioritizes issues (critical vs nice-to-have)
      - Maintains a constructive, professional tone
      - Balances criticism with acknowledgment of good practices

      Score based on:
      - 5: Exceptional feedback that educates and guides improvement
      - 4: Good feedback with clear priorities and recommendations
      - 3: Adequate feedback but missing depth or prioritization
      - 2: Generic feedback without specific guidance
      - 1: Unhelpful or unconstructive feedback
    files:
      - "review_output.txt"
    use_last_response: true
    weight: 0.25

# Judge configuration
judge:
  model: opus
  use_cot: true
  temperature: 0
  system_prompt: |
    You are evaluating the quality of a CODE REVIEW performed by an AI assistant.
    Focus on how well the review would help a developer improve their code.

    The review was of a Python CLI calculator implementation.

    Score each metric on a 1-5 scale according to the provided criteria.
    Provide specific examples from the review to justify each score.

    Output your response as JSON with structure:
    {
      "scores": {
        "metric_name": {
          "score": N,
          "justification": "Brief explanation with specific examples from the review"
        }
      }
    }

# Acceptance criteria for this eval
acceptance:
  # Minimum mean score difference between variants to be considered significant
  minimum_mean_difference: 0.3
  # Statistical significance threshold (p-value)
  significance_threshold: 0.05
  # Minimum passing rate for binary checks
  binary_pass_threshold: 0.7

# Metadata for reporting
metadata:
  category: agent
  target_agent: code-reviewer
  comparison_type: agent_vs_direct
  task_type: code_review
  complexity: intermediate
  estimated_duration_minutes: 5
  tags:
    - code-review
    - security
    - quality-gates
    - owasp
    - feedback
  related_agents:
    - verify-app
    - code-simplifier
  related_rubrics:
    - code-quality.md
