# Eval spec for component isolation: Agents Impact Assessment
# Purpose: Isolate and measure the impact of agent delegation on code quality
# Compares: Framework with agents vs framework without agents (skills still enabled)
#
# This spec differs from agent-specific evals by testing the cumulative
# impact of the agent delegation system, not individual agent effectiveness.

name: agents-impact
version: 2.0.0
description: |
  Component isolation eval to measure the aggregate impact of agent delegation on code quality.

  Both variants have access to skills, but only the treatment variant has agents enabled.
  This isolates the agent delegation contribution to overall quality.

  Uses variant-based fixture structure for proper A/B testing:
  - with_agents variant: Full framework with agent delegation enabled
  - without_agents variant: Framework with skills but agent delegation disabled

# Test fixture configuration (per-variant fixture paths)
fixture:
  repo: ensemble-vnext-test-fixtures
  # Note: path is now per-variant, see variants[].fixture_path

# Test case definition
test_case:
  base_prompt: |
    Implement a REST API endpoint for managing user profiles using FastAPI.

    Requirements:
    - POST /profiles - Create a new profile
    - GET /profiles/{id} - Get profile by ID
    - PUT /profiles/{id} - Update profile
    - DELETE /profiles/{id} - Delete profile

    Profile model:
    - id: auto-generated UUID
    - name: required string
    - email: required string with validation
    - bio: optional string (max 500 chars)
    - created_at: timestamp

    Implementation requirements:
    - Use Pydantic models for validation
    - Include proper error handling (404, 422)
    - Write comprehensive pytest tests
    - Follow FastAPI best practices

    Workflow:
    1. Implement the API endpoints
    2. Run tests to verify implementation
    3. Review code for quality improvements
    4. Refactor if needed, then re-run tests

# Variants to compare using variant-based fixture structure
variants:
  - id: with_agents
    name: With Agents
    description: Full framework with agent delegation enabled
    fixture_path: variants/full/fastapi-profiles
    suffix: |
      Use the full Ensemble workflow with agent delegation:

      1. Delegate implementation to @backend-implementer
      2. Delegate verification to @verify-app
      3. Delegate code review to @code-reviewer
      4. Delegate simplification to @code-simplifier (if needed)

      Use the developing-with-python skill for Python patterns.
      Report which agents you delegated to and their results.
    skill_enabled: true
    agent_enabled: true
    timeout_seconds: 600
    expected_agents:
      - backend-implementer
      - verify-app
      - code-reviewer
    expected_skills:
      - developing-with-python

  - id: without_agents
    name: Without Agents (Skills Only)
    description: Framework with skills enabled but agent delegation disabled
    fixture_path: variants/baseline/fastapi-profiles
    is_baseline: true
    suffix: |
      Complete all steps directly without delegating to any agents.

      You have access to skills - use developing-with-python for Python patterns.

      1. Implement the API endpoints directly
      2. Run pytest to verify implementation
      3. Review the code yourself for quality
      4. Refactor if needed, then re-run tests

      Do NOT use @backend-implementer, @verify-app, @code-reviewer, or any other agents.
    skill_enabled: true
    agent_enabled: false
    timeout_seconds: 600
    expected_agents: []
    expected_skills:
      - developing-with-python

# Execution configuration
runs_per_variant: 5

# Automated binary checks (shell-based, exit 0 on success)
binary_checks:
  - name: implementation_complete
    description: FastAPI endpoints were implemented
    check: |
      cd {workspace} && \
        [ -f main.py ] || [ -f app.py ] || [ -f api.py ] && \
        grep -rq "@app\.\|@router\." *.py 2>/dev/null
    weight: 0.25

  - name: all_endpoints
    description: All four CRUD endpoints are defined
    check: |
      cd {workspace} && \
        count=$(grep -rE '@(app|router)\.(get|post|put|delete)\(' *.py 2>/dev/null | wc -l)
        [ "$count" -ge 4 ]
    weight: 0.25

  - name: tests_pass
    description: Tests execute successfully
    check: |
      cd {workspace} && python -m pytest -v --tb=short 2>&1
    weight: 0.25
    critical: true

  - name: pydantic_models
    description: Pydantic models are used for validation
    check: |
      cd {workspace} && \
        grep -rq "class.*BaseModel\|from pydantic" *.py 2>/dev/null
    weight: 0.25

# LLM-judged quality metrics
metrics:
  - name: code_quality
    description: Overall code quality assessment
    rubric: code-quality.md
    files:
      - "*.py"
      - "!test_*.py"
      - "!*_test.py"
      - "!conftest.py"
    weight: 0.4
    custom_prompt_addition: |
      Focus on FastAPI-specific quality aspects:
      - Is the router/app structure well-organized?
      - Are Pydantic models properly designed?
      - Is dependency injection used appropriately?
      - Are type hints complete?

  - name: error_handling
    description: Error handling quality assessment
    rubric: error-handling.md
    files:
      - "*.py"
      - "!conftest.py"
    weight: 0.3
    custom_prompt_addition: |
      Evaluate error handling:
      - Are HTTPExceptions used with appropriate status codes?
      - Are validation errors handled properly?
      - Is the error response format consistent?

  - name: test_quality
    description: Test coverage and structure assessment
    rubric: test-quality.md
    files:
      - "test_*.py"
      - "*_test.py"
    context_files:
      - "*.py"
      - "!test_*.py"
    weight: 0.3
    custom_prompt_addition: |
      Evaluate test quality:
      - Are all endpoints tested?
      - Are error cases covered (404, 422)?
      - Is TestClient used correctly?
      - Are fixtures used effectively?

# Judge configuration
judge:
  model: opus
  use_cot: true
  temperature: 0
  system_prompt: |
    You are evaluating FastAPI code quality for a profile management API.
    This is a COMPONENT ISOLATION test - we are specifically measuring
    whether agent delegation contributes to better code quality when skills
    are available in both conditions.

    Score each metric on a 1-5 scale according to the provided rubric.
    Output JSON: {"scores": {"metric_name": {"score": N, "justification": "..."}}}

# Session log analysis for agent verification
session_analysis:
  verify_backend_delegation:
    pattern: "backend-implementer"
    description: Verify @backend-implementer was delegated to (with_agents variant)
  verify_verify_app_delegation:
    pattern: "verify-app"
    description: Verify @verify-app was delegated to (with_agents variant)
  verify_code_reviewer_delegation:
    pattern: "code-reviewer"
    description: Verify @code-reviewer was delegated to (with_agents variant)

# Acceptance criteria for this eval
acceptance:
  minimum_mean_difference: 0.3
  significance_threshold: 0.05
  binary_pass_threshold: 0.8
  critical_checks_required: true

# Metadata for reporting
metadata:
  category: component
  component_tested: agents
  isolation_type: agents-only
  language: python
  framework: FastAPI
  complexity: intermediate
  estimated_duration_minutes: 10
  tags:
    - component-isolation
    - agents-impact
    - fastapi
    - python
    - delegation
    - a-b-test
