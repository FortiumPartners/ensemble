# Eval spec for component isolation: Skills Impact Assessment
# Purpose: Isolate and measure the impact of skills on code quality
# Compares: Framework with skills vs framework without skills (agents still enabled)
#
# This spec differs from skill-specific evals by testing the cumulative
# impact of the skill system, not individual skill effectiveness.

name: skills-impact
version: 2.0.0
description: |
  Component isolation eval to measure the aggregate impact of skills on code quality.

  Both variants have access to agents, but only the treatment variant has skills enabled.
  This isolates the skills system contribution to overall quality.

  Uses variant-based fixture structure for proper A/B testing:
  - with_skills variant: Full framework with skills enabled
  - without_skills variant: Framework with agents but skills disabled

# Test fixture configuration (per-variant fixture paths)
fixture:
  repo: ensemble-vnext-test-fixtures
  # Note: path is now per-variant, see variants[].fixture_path

# Test case definition
test_case:
  base_prompt: |
    Implement a Python utility module with the following functions:

    1. validate_email(email: str) -> bool
       - Check if email format is valid
       - Use regex or email-validator library

    2. sanitize_html(text: str) -> str
       - Remove HTML tags from input
       - Preserve plain text content

    3. format_currency(amount: float, currency: str = "USD") -> str
       - Format number as currency string
       - Support USD, EUR, GBP

    Requirements:
    - Include comprehensive type hints
    - Write pytest tests for all functions
    - Handle edge cases (empty strings, None, invalid input)
    - Follow Python best practices

    Create:
    - utils.py with the three functions
    - test_utils.py with comprehensive tests

# Variants to compare using variant-based fixture structure
variants:
  - id: with_skills
    name: With Skills
    description: Full framework with skills enabled
    fixture_path: variants/full/python-utils
    suffix: |
      Use the Skill tool to invoke the developing-with-python skill.
      After loading the skill, follow its patterns and best practices.

      Also use the pytest skill for test implementation guidance.
    skill_enabled: true
    agent_enabled: true
    timeout_seconds: 300
    expected_skills:
      - developing-with-python
      - pytest

  - id: without_skills
    name: Without Skills (Framework Only)
    description: Framework with agents enabled but skills disabled
    fixture_path: variants/baseline/python-utils
    is_baseline: true
    suffix: |
      Implement directly without loading any skills.
      You still have access to agents if needed, but do NOT use the Skill tool.
    skill_enabled: false
    agent_enabled: true
    timeout_seconds: 300
    expected_skills: []

# Execution configuration
runs_per_variant: 5

# Automated binary checks (shell-based, exit 0 on success)
binary_checks:
  - name: implementation_complete
    description: Both source and test files were created
    check: |
      cd {workspace} && \
        [ -f utils.py ] && \
        grep -q "def validate_email\|def sanitize_html\|def format_currency" utils.py && \
        [ -f test_utils.py ]
    weight: 0.3

  - name: tests_pass
    description: All tests execute successfully with pytest
    check: |
      cd {workspace} && python -m pytest test_utils.py -v --tb=short 2>&1
    weight: 0.4

  - name: type_hints_present
    description: Functions have type hints
    check: |
      count=$(grep -E 'def .+\(.+:.+\).*->' {workspace}/utils.py 2>/dev/null | wc -l)
      [ "$count" -ge 3 ]
    weight: 0.3

# LLM-judged quality metrics
metrics:
  - name: code_quality
    description: Overall code quality assessment
    rubric: code-quality.md
    files:
      - "utils.py"
    weight: 0.5
    custom_prompt_addition: |
      Focus on Python-specific quality aspects:
      - Are type hints complete and accurate?
      - Is error handling comprehensive?
      - Are edge cases handled properly?
      - Is the code idiomatic Python?

  - name: test_quality
    description: Test coverage and structure assessment
    rubric: test-quality.md
    files:
      - "test_utils.py"
    context_files:
      - "utils.py"
    weight: 0.5
    custom_prompt_addition: |
      Evaluate test comprehensiveness:
      - Are all three functions tested?
      - Are edge cases covered?
      - Are pytest fixtures and parametrize used effectively?
      - Is test organization logical?

# Judge configuration
judge:
  model: opus
  use_cot: true
  temperature: 0
  system_prompt: |
    You are evaluating Python code quality for a utility module.
    This is a COMPONENT ISOLATION test - we are specifically measuring
    whether skills contribute to better code quality when agents are
    available in both conditions.

    Score each metric on a 1-5 scale according to the provided rubric.
    Output JSON: {"scores": {"metric_name": {"score": N, "justification": "..."}}}

# Acceptance criteria for this eval
acceptance:
  minimum_mean_difference: 0.3
  significance_threshold: 0.05
  binary_pass_threshold: 0.8

# Metadata for reporting
metadata:
  category: component
  component_tested: skills
  isolation_type: skills-only
  language: python
  complexity: intermediate
  estimated_duration_minutes: 5
  tags:
    - component-isolation
    - skills-impact
    - python
    - pytest
    - a-b-test
