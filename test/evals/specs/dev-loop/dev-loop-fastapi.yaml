# Eval spec for development loop testing: implement -> verify -> simplify -> verify
# Tests the complete development cycle with FastAPI Todo API
# Fixture: ensemble-vnext-test-fixtures with variant-based structure

name: dev-loop-fastapi
version: 3.0.0
description: |
  Test the complete development loop cycle for FastAPI endpoint implementation:

  1. IMPLEMENT: Delegate to @backend-implementer to build from story.md Todo List API with Pydantic models
  2. VERIFY: Delegate to @verify-app to run tests and report coverage
  3. SIMPLIFY: Delegate to @code-simplifier to refactor while preserving tests
  4. VERIFY: Delegate to @verify-app again to confirm tests still pass

  This eval measures whether agent delegation improves the quality and
  consistency of the full development workflow for FastAPI applications.

  Three test variants:
  - baseline: Vanilla Claude without framework assistance
  - framework: Full Ensemble with agents and skills (implement + verify + simplify)
  - full-workflow: PRD -> TRD -> implement-trd orchestrated workflow

# Test fixture configuration (per-variant fixture paths)
fixture:
  repo: ensemble-vnext-test-fixtures
  # Note: path is now per-variant, see variants[].fixture_path

# Test case definition
test_case:
  base_prompt: |
    Complete this task using the full development loop:

    ## STEP 1 - IMPLEMENT
    Read the story.md file and implement a Todo List API with FastAPI.

    Requirements:
    - GET /todos - List all todos
    - POST /todos - Create a new todo
    - PUT /todos/{id} - Update a todo
    - DELETE /todos/{id} - Delete a todo
    - Use Pydantic models for request/response validation
    - Include proper HTTP status codes (200, 201, 404, 422)
    - Use type hints throughout

    ## STEP 2 - VERIFY (Initial)
    Run tests to verify the implementation works correctly.
    Report test results and coverage metrics.
    Target: >= 80% unit test coverage.

    ## STEP 3 - SIMPLIFY
    Review the code for potential simplifications:
    - Reduce complexity where possible
    - Improve readability and organization
    - Extract common patterns
    - Ensure proper separation of concerns

    ## STEP 4 - VERIFY (Final)
    Run tests again to confirm all tests still pass after simplification.
    Report final coverage metrics.

# Variants to compare using variant-based fixture structure
variants:
  - id: baseline
    name: Vanilla Claude (No Framework)
    description: Complete all steps directly without framework assistance
    fixture_path: variants/baseline/fastapi-endpoint
    is_baseline: true
    suffix: |
      Implement directly without any framework assistance.

      Complete all steps directly without delegating to any agents.

      For STEP 1 (IMPLEMENT):
        Implement the FastAPI Todo API directly based on story.md requirements.

      For STEP 2 and STEP 4 (VERIFY):
        Run pytest directly: python -m pytest -v --cov --cov-report=term
        Test each endpoint: GET, POST, PUT, DELETE
        Report pass/fail counts and coverage percentage.

      For STEP 3 (SIMPLIFY):
        Apply refactoring patterns directly to improve code quality.
        Ensure tests still pass after changes.

      Do not use any agents, skills, or commands from the framework.
    agent_enabled: false
    skill_enabled: false
    timeout_seconds: 600
    expected_agents: []
    expected_skills: []

  - id: framework
    name: Full Ensemble Framework with Agent Delegation
    description: Use agent delegation pattern for ALL steps including implementation
    fixture_path: variants/full/fastapi-endpoint
    suffix: |
      Use the full Ensemble workflow with agent delegation for EVERY step.

      For STEP 1 (IMPLEMENT):
        Delegate to @backend-implementer agent for initial implementation.
        Request: Read story.md and implement the FastAPI Todo API with:
        - CRUD endpoints (GET, POST, PUT, DELETE)
        - Pydantic models for validation
        - Comprehensive test coverage

      For STEP 2 and STEP 4 (VERIFY):
        Delegate to @verify-app agent for test execution.
        Request: Run pytest with coverage, test all API endpoints.

      For STEP 3 (SIMPLIFY):
        Delegate to @code-simplifier agent for refactoring.
        Request: Simplify the FastAPI implementation while preserving API behavior.

      Use the appropriate skills:
      - developing-with-python for Python/FastAPI patterns
      - pytest for test execution

      Report which agents and skills you used at each step.
    agent_enabled: true
    skill_enabled: true
    timeout_seconds: 600
    expected_agents:
      - backend-implementer
      - verify-app
      - code-simplifier
    expected_skills:
      - developing-with-python
      - pytest

  - id: full-workflow
    name: Full PRD -> TRD -> implement-trd Workflow
    description: Use the complete structured workflow with upfront design
    fixture_path: variants/full/fastapi-endpoint
    suffix: |
      Use the full structured workflow with upfront design artifacts.

      BEFORE implementing, create the design artifacts:

      1. Run /create-prd to generate a Product Requirements Document
         Input: Use the story.md as the product description.
         Output: docs/PRD/fastapi-endpoint.md

      2. Run /create-trd to generate a Technical Requirements Document
         Input: Use the PRD just created.
         Output: docs/TRD/fastapi-endpoint.md

      3. Run /implement-trd to execute the implementation
         This should orchestrate the full dev loop with TDD methodology:
         - Use @backend-implementer for implementation
         - Use @verify-app for test verification
         - Use @code-simplifier for refactoring
         - Track progress in .trd-state/

      The implement-trd command should handle STEPS 1-4 automatically using
      the TRD execution plan and agent delegation.

      Report which commands, agents, and skills were used throughout.
    agent_enabled: true
    skill_enabled: true
    timeout_seconds: 1200
    expected_agents:
      - product-manager
      - technical-architect
      - backend-implementer
      - verify-app
      - code-simplifier
    expected_skills:
      - developing-with-python
      - pytest
    expected_commands:
      - create-prd
      - create-trd
      - implement-trd

# Execution configuration
runs_per_variant: 3

# Automated binary checks (shell-based, exit 0 on success)
binary_checks:
  - name: implementation_complete
    description: FastAPI application files were created
    check: |
      cd {workspace} && \
        [ -f main.py ] && \
        grep -q "FastAPI\|fastapi" main.py && \
        grep -q "BaseModel\|Pydantic\|pydantic" main.py && \
        [ -f test_main.py ]
    weight: 0.25

  - name: tests_pass_initial
    description: Tests pass after initial implementation
    check: |
      cd {workspace} && \
        python -m pytest test_main.py -v --tb=short 2>&1
    weight: 0.25

  - name: tests_pass_final
    description: Tests pass after simplification (behavior preserved)
    check: |
      cd {workspace} && \
        python -m pytest test_main.py -v --tb=short 2>&1
    weight: 0.25
    critical: true

  - name: api_endpoints_exist
    description: All required API endpoints are implemented
    check: |
      cd {workspace} && \
        grep -q "get.*todos\|@.*get.*\/todos" main.py && \
        grep -q "post.*todos\|@.*post.*\/todos" main.py && \
        grep -q "put.*todos\|@.*put.*\/todos" main.py && \
        grep -q "delete.*todos\|@.*delete.*\/todos" main.py
    weight: 0.25

# LLM-judged quality metrics
metrics:
  - name: code_quality
    description: Overall code quality of the FastAPI implementation
    rubric: code-quality.md
    files:
      - "main.py"
      - "models.py"
    context_files:
      - "story.md"
    weight: 0.4
    custom_prompt_addition: |
      Focus on FastAPI-specific quality aspects:
      - Are Pydantic models well-defined with appropriate validation?
      - Is there proper separation between routes and business logic?
      - Are HTTP status codes used correctly?
      - Is dependency injection used appropriately?
      - Are type hints comprehensive?
      - Is error handling consistent (HTTPException usage)?

  - name: test_quality
    description: Quality of the API test suite
    rubric: test-quality.md
    files:
      - "test_main.py"
    context_files:
      - "main.py"
    weight: 0.3
    custom_prompt_addition: |
      Evaluate FastAPI testing patterns:
      - Is TestClient used for API testing?
      - Are all endpoints tested (GET, POST, PUT, DELETE)?
      - Are error cases tested (404, 422)?
      - Are Pydantic validation errors tested?
      - Are response models validated?

  - name: simplification_quality
    description: Quality of the refactoring/simplification step
    rubric: code-quality.md
    files:
      - "main.py"
    weight: 0.3
    custom_prompt_addition: |
      Focus on whether refactoring improved the FastAPI application:
      - Was route logic simplified?
      - Were common patterns extracted (e.g., get_todo_or_404)?
      - Is the code structure cleaner?
      - Was over-engineering avoided?

# Judge configuration
judge:
  model: opus
  use_cot: true
  temperature: 0
  system_prompt: |
    You are evaluating a complete development loop for a FastAPI Todo API:
    implement -> verify -> simplify -> verify

    Consider FastAPI-specific best practices:
    1. Pydantic model design and validation
    2. Proper HTTP status codes and error handling
    3. TestClient usage for API testing
    4. Clean route organization

    Score each metric on a 1-5 scale according to the provided rubric.
    Output JSON: {"scores": {"metric_name": {"score": N, "justification": "..."}}}

# Session log analysis for agent verification
session_analysis:
  verify_verify_app_delegation:
    pattern: "verify-app"
    description: Verify @verify-app agent was delegated to (with_agents variant)
  verify_code_simplifier_delegation:
    pattern: "code-simplifier"
    description: Verify @code-simplifier agent was delegated to (with_agents variant)
  verify_fastapi_patterns:
    pattern: "FastAPI|TestClient|Pydantic"
    description: Verify FastAPI patterns were used

# Acceptance criteria for this eval
acceptance:
  minimum_mean_difference: 0.3
  significance_threshold: 0.05
  binary_pass_threshold: 0.8
  critical_checks_required: true

# Metadata for reporting
metadata:
  category: dev-loop
  target_workflow: implement-verify-simplify-verify
  language: python
  tech_stack: fastapi
  complexity: intermediate
  estimated_duration_minutes: 10
  tags:
    - development-loop
    - python
    - fastapi
    - pydantic
    - api
    - pytest
    - agent-comparison
